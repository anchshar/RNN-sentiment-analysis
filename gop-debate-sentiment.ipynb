{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6935\n",
      "Done reading data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_data = pd.read_csv('GOP_REL_ONLY.csv')\n",
    "mid = int(len(csv_data) / 2)\n",
    "\n",
    "train = pd.DataFrame(list(zip(csv_data['text'][:mid], csv_data['sentiment'][:mid] )), columns=['text', 'sentiment'])\n",
    "test =  pd.DataFrame(list(zip(csv_data['text'][mid+1:], csv_data['sentiment'][mid+1:] )), columns=['text', 'sentiment'])\n",
    "\n",
    "print(len(train['text']))\n",
    "print(\"Done reading data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# words found originally :: 16158\n",
      "# filtered words :: 5984\n",
      "Max tweet length :: 41\n",
      "                                                text sentiment  \\\n",
      "2  RT @TJMShow: No mention of Tamir Rice and the ...   Neutral   \n",
      "\n",
      "                                              tokens  \\\n",
      "2  [rt, tjmshow, no, mention, of, tamir, rice, an...   \n",
      "\n",
      "                                             vectors  \n",
      "2  [2, 1, 32, 33, 24, 34, 35, 36, 9, 16, 37, 38, ...  \n",
      "Done vectorizing input data\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "train['tokens'] = train['text'].map(lambda x: re.sub(r\"[^a-zA-Z0-9 ]+\", '', x.lower()).split(' '))\n",
    "test['tokens'] = test['text'].map(lambda x: re.sub(r\"[^a-zA-Z0-9 ]+\", '', x.lower()).split(' '))\n",
    "\n",
    "word2i = {}\n",
    "i2word = {}\n",
    "filtered_words = ['<UNK>']\n",
    "freq_word = {}\n",
    "min_freq = 2\n",
    "\n",
    "def count_freq(x):\n",
    "    global freq_word\n",
    "    if x in freq_word:\n",
    "        freq_word[x] = freq_word[x] + 1\n",
    "    else:\n",
    "        freq_word[x] = 1\n",
    "    return x\n",
    "\n",
    "def freq_filter(x):\n",
    "    global word2i\n",
    "    global i2word\n",
    "    global filtered_words\n",
    "    global freq_word\n",
    "    global min_freq\n",
    "    \n",
    "    if x in freq_word and freq_word[x] >= min_freq:\n",
    "        filtered_words.append(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "train['tokens'].map(lambda tweet: [count_freq(word) for word in tweet])\n",
    "#test['tokens'].map(lambda seq: [count_freq(word) for word in seq])\n",
    "\n",
    "df_freq = pd.DataFrame(list(zip( freq_word.keys(), freq_word.items() )), columns=['word','count'])\n",
    "df_freq['word'].map(lambda word: freq_filter(word))\n",
    "\n",
    "\n",
    "lookup_len = len(filtered_words)\n",
    "print('# words found originally :: ' + str(len(freq_word)))\n",
    "print('# filtered words :: ' + str(lookup_len))\n",
    "\n",
    "i2word = dict(zip(list(range(1,lookup_len + 1)), filtered_words))\n",
    "word2i = dict(zip(filtered_words, list(range(1,lookup_len + 1))))\n",
    "\n",
    "train['vectors'] = train['tokens'].map(lambda tweet: [word2i[word] if word in word2i else 1 for word in tweet])\n",
    "test['vectors'] = test['tokens'].map(lambda tweet: [word2i[word] if word in word2i else 1 for word in tweet])\n",
    "\n",
    "max_tweet_len = max( len(tweet) for tweet in train['tokens'])\n",
    "max_tweet_len = max( len(tweet) for tweet in test['tokens'])\n",
    "\n",
    "print('Max tweet length :: ' + str(max_tweet_len))\n",
    "\n",
    "print(train[2:3])\n",
    "\n",
    "print(\"Done vectorizing input data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded training data shape ::: (6935, 41)\n",
      "# Classes :: 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     (None, 41)                0         \n",
      "_________________________________________________________________\n",
      "embedding_layer (Embedding)  (None, 41, 50)            299250    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 41, 80)            31440     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 41, 80)            38640     \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 80)                38640     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 243       \n",
      "=================================================================\n",
      "Total params: 408,213\n",
      "Trainable params: 408,213\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "6935/6935 [==============================] - 14s 2ms/step - loss: 0.4594\n",
      "Epoch 2/5\n",
      "6935/6935 [==============================] - 13s 2ms/step - loss: 0.3958\n",
      "Epoch 3/5\n",
      "6935/6935 [==============================] - 13s 2ms/step - loss: 0.3614\n",
      "Epoch 4/5\n",
      "6935/6935 [==============================] - 13s 2ms/step - loss: 0.3144\n",
      "Epoch 5/5\n",
      "6935/6935 [==============================] - 13s 2ms/step - loss: 0.2858\n",
      "Done training\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from pathlib import Path\n",
    "\n",
    "padded_tweets = pad_sequences(sequences = train['vectors'], maxlen = max_tweet_len)\n",
    "x_train = np.matrix(padded_tweets)\n",
    "\n",
    "print('Padded training data shape ::: ' + str(x_train.shape))\n",
    "\n",
    "y_train = le.fit_transform(train['sentiment'])\n",
    "\n",
    "n_classes = len(set(y_train))\n",
    "\n",
    "print('# Classes :: ' + str(n_classes))\n",
    "\n",
    "n_embedding = 50\n",
    "n_gru = 80\n",
    "\n",
    "def get_model(n_embedding, n_gru, seq_len, vocab_dim, batch_size=None, stateful=False):\n",
    "    \n",
    "    input_layer = Input(batch_shape=(batch_size, seq_len), name='input_layer')\n",
    "    # batch_size * seq_len\n",
    "    \n",
    "    embedding_layer = Embedding(input_dim=vocab_dim, output_dim=n_embedding, mask_zero=True,\n",
    "                                name='embedding_layer')(input_layer)\n",
    "    # batch_size * seq_len * n_embedding\n",
    "    \n",
    "    gru_1 = GRU(n_gru, return_sequences=True, stateful=stateful, activation='relu',\n",
    "                name='gru_1')(embedding_layer)\n",
    "    # batch_size * seq_len * n_gru\n",
    "    \n",
    "    gru_2 = GRU(n_gru, return_sequences=True, stateful=stateful, activation='relu',\n",
    "                name='gru_2')(gru_1)\n",
    "    # batch_size * seq_len * n_gru\n",
    "    \n",
    "    gru_3 = GRU(n_gru, return_sequences=False, stateful=stateful, activation='relu',\n",
    "                name='gru_3')(gru_2)\n",
    "    # batch_size * n_gru\n",
    "    \n",
    "    output_layer = Dense(3, activation='softmax')(gru_3)\n",
    "    # batch_size * 3\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = get_model(n_embedding=n_embedding, seq_len=max_tweet_len, n_gru=n_gru,\n",
    "                  vocab_dim=lookup_len + 1)\n",
    "print(model.summary())\n",
    "\n",
    "file_path = 'gop_weights' + '.embed=' + str(n_embedding) + '.gru=' + str(n_gru) + '.h5'\n",
    "init_weights_file = Path(file_path)\n",
    "\n",
    "if init_weights_file.exists():\n",
    "    model.load_weights(file_path)\n",
    "\n",
    "model.fit(x = x_train, y = y_train, epochs=5, batch_size=100)\n",
    "model.save_weights(file_path)\n",
    "\n",
    "print(\"Done training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape :: (6935, 41)\n",
      "[1, 0, 0, 1, 0, 1, 0, 0, 1]\n",
      "[0 0 0 2 0 1 0 1 0]\n",
      "Accuracy :: 0.5495313626532083\n",
      "Done testing\n"
     ]
    }
   ],
   "source": [
    "pred_model = get_model(n_embedding=n_embedding, seq_len=max_tweet_len, n_gru=n_gru,\n",
    "                  vocab_dim=lookup_len + 1)\n",
    "pred_model.load_weights(file_path)\n",
    "\n",
    "padded_tweets = pad_sequences(sequences = test['vectors'], maxlen = max_tweet_len)\n",
    "x_test = np.matrix(padded_tweets)\n",
    "y_test = le.fit_transform(test['sentiment'])\n",
    "\n",
    "print('Test data shape :: ' + str(x_test.shape))\n",
    "\n",
    "res = pred_model.predict(x=x_test)\n",
    "res = [np.argmax(arr) for arr in res]\n",
    "\n",
    "print(res[1:10])\n",
    "print(y_test[1:10])\n",
    "\n",
    "hits = np.sum(res == y_test)\n",
    "print('Accuracy :: ' + str( float(hits/len(y_test))))\n",
    "\n",
    "print('Done testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
